\begin{savequote}[100mm]
...ZooKeeper, a service for coordinating processes of distributed applications. Since ZooKeeper is part of critical infrastructure, ZooKeeper aims to provide a simple and high performance kernel for building more complex coordination primitives at the client.
\qauthor{---Authors of \textup{ZooKeeper: Wait-free coordination for Internet-scale systems}}
\end{savequote}

\chapter{Zookeeper}
In the simplest of terms ZooKeeper is a distributed service with the goal to assist in coordination and synchronization between its clients.

\section{ZooKeeper Architecture}

Figure ~\ref{fig:ZooKeeperArchitecture} depicts the architecture of a 3-server ZooKeeper cluster. There are three nodes\textit{(Server 1, Server 2, Server 3)} which accept requests from clients to store, update and delete data. These nodes cooperate with each other and coordinate the writes so that they contain identical copies. These nodes are called as ZooKeeper servers. When a ZooKeeper server is started it has to be provided with an id and also the addresses of the other ZooKeeper servers. All the servers should contain an identical copy of the id and the address information.

The servers keep their respective datastores in memory which restricts the amount of data which can be stored. The servers also keep a log of the transactions which were performed so that the data can be recovered in case of failures.

\addvspace{2.5em}
\input{diagrams/zookeeper}
\addvspace{1.0em}

The ZooKeeper servers communicate with each other using a consensus protocol called ZAB~\cite{junqueira2011zab} which is modeled on Paxos but is not entirely similar. When a request is received on the servers the request is forwarded to the leader which computes the necessary changes and then broadcasts it to the rest of the servers. When a majority of the servers respond with an acknowledgment the transaction can be committed and a response can be sent to the client. 

The size of the quorum is the factor against which performance is measured. Three is the smallest number of servers required for a quorum. One failure is tolerated but both the other two servers must be available. This setup generally gives the fastest throughput. A quorum needs three or more odd number of servers. Five servers is the next largest and seven and nine come after that. Quorums larger than these are generally not used in production.

For requests which do not modify the state of the server like data read requests, there is no need for consensus. All the servers contain identical copies of the data and any read request can be processed locally on the server. 

The clients are initialized with the addresses of all or some of the addresses of the servers in the ZooKeeper cluster. The client then chooses one of the servers and tries to connect to it through a TCP connection. If the connection cannot be made or the connection breaks during operation then the client attempts to connect to another server.

\subsection{Client Sessions}
When a ZooKeeper client connects to a ZooKeeper cluster this creates a session associated with that instance of the connection. Whenever the client loses the connection to the server it attempts to reconnect to another server in the cluster. If it cannot reconnect within a specified amount of time then the session associated with that client connection is closed. When the client subsequently connects at some point in the future then a new session is created. Any locks which are held by a client are deleted when the session is lost. The server periodically sends a pulse to the client to check if it is still alive. If it does not receive an acknowledgment it resends the pulse again. Finally after a preconfigured timeout the client is considered to be lost and the session is closed.

\subsection{ZooKeeper Data Organization}
ZooKeeper data is presented like a UNIX file system. There are no files and directories, only \textit{znodes}. Every znode can have data associated with it and also have children. Other metadata about the znode like the creation timestamp, the owner of the znode, the version of the data and any access-control information is also stored along with the data. Figure ~\ref{fig:ZKDataOrganization} shows a part of tree typical in ZooKeeper cluster. The circle at the top represents the root node and always has the path \textit{/}. In this example the root node has two child nodes viz. \textit{/app1} and \textit{/app2}. the znode \textit{/app1} in turn has 3 children called \textit{p\_1}, \textit{p\_2} and \textit{p\_3}. These are leaf nodes and are represented as hexagons. The full path for znode \textit{p\_1} would be \textit{/app1/p\_1}.

\vspace{1.5em}
\input{diagrams/zookeeper_data}
\vspace{1.5em}

ZooKeeper has two types of znodes, regular and ephemeral znodes. Ephemeral znodes are present only as long at the client which created them is connected. If the connection drops or the client crashes causing the connection to drop the the ephemeral znode is deleted. Ephemeral znodes also are different because they cannot have child znodes. Attempting to create a child znode for an ephemeral znode throws an exception. Regular znodes are persisted across sessions. There are also sequential znodes which have a counter appended to their names. Sequential znodes are created by setting the \textit{sequential} flag to true while creating the node. All sequential nodes which are siblings and have the same name have an incrementing value appended to the name as sequential znodes are created. Znodes can also be both sequential and ephemeral in which case the znodes have a counter value appended to the end of names and they are deleted when the client which created them is disconnected.

\subsection{ZooKeeper Guarantees}
ZooKeeper has two ordering guarantees:
\begin{enumerate}
	\item \textbf{Linearisable Writes:} All the update operations on the cluster can be serialised and they respect precedence.
	\item \textbf{Client FIFO Order:} All operations from a client are executed in the order they were sent from the client.
\end{enumerate}
ZooKeeper also has the following \textit{Liveness} and \textit{Durability} guarantees: As long as a quorum of servers are available the service can accept, process and commit requests. Any previously committed changes will be always present as long as a quorum of servers are available.

\subsection{Cluster Reconfiguration}
	The current version of ZooKeeper cannot be reconfigured dynamically. To add more servers to a cluster the cluster has to be first stopped. The new servers have to be added to the configuration of the cluster members and then finally the cluster has to be restarted. This is a manual process which is prone to errors and can cause down-time due to misconfiguration or split brain. This entails downtime. But it is also important that new servers be added dynamically to balance load and to replaces servers which have crashed. By utilizing some of the properties of the ZooKeeper system a reconfiguration method~\cite{shraer2012dynamic} was proposed which can rebalance the clients dynamically and migrate the data without shutting down the cluster.

\subsection{Why not a Distributed Database?}
At a first glance, it may appear that a Distributed Database like Cassandra or BigTable, HBase may solve this coordination problem. However these databases are actually built using a distributed co-ordination service like ZooKeeper. ZooKeeper which is maintained by the Apache Foundation is used as a component in Cassandra. Similarly Chubby which was developed as a co-ordination service by Google is used in GFS and BigTable.

\section{Kazoo Library}
Since the connections are made through TCP there are several languages which have libraries to communicate with ZooKeeper. The Python~\cite{van2002python} library which is provided with the ZooKeeper package is quite small and contains limited features. Kazoo is a Python library which contains many features which are borrowed from Netflix's Curator. It is a pure Python implementation which means that it doesn't have any external C dependencies. It has support for older version of ZooKeeper viz. 3.4 and 3.3. It has also support for evented IO through gevent. It also provides many higher order primitives like Watchers, Locks, Barriers. For these reason Kazoo is used as the base for the ParKazoo library. ParKazoo tries to maintain the same API as Kazoo. It can be in fact for most applications be used as a drop-in replacement for Kazoo.

\subsection{Library API}
The Kazoo library provides the following functions. Some of these are also present in the standard Python library included in the ZooKeeper package. They are:
  \subsubsection{Initialization of Kazoo}
  	\begin{lstlisting}
  		def __init__(hosts, timeout=10.0, randomize_hosts=True)
  	\end{lstlisting}

  \subsubsection{Sync Operations}
  	\begin{lstlisting}
  		def sync(path)
  	\end{lstlisting}

  \subsubsection{Server Commands}
  	\begin{lstlisting}
  		def command(cmd='ruok')
  	\end{lstlisting}


  \subsubsection{Create Node}
  \begin{lstlisting}
    def create(path, value, ephemeral=True, sequential=True, make_path=False, watch=None)
  \end{lstlisting}
  This call creates a znode in the tree structure. The \textit{path} argument indicates the path of the node, the value is byte array containing the data to be stored. \textit{make\_path} indicates if the the ancestor znodes should also be created if they do not yet exists. The watch parameter leaves a watch on the node which calls a function when the node is modified. The type of change event and the corresponding new data is passed as an argument to the watch function. The \textit{sequential} and \textit{ephemeral} flags indicate if the znode to be created should be of those corresponding types.
  
  \subsubsection{Read a Node}
  \begin{lstlisting}
    def get(path, watch=None)
  \end{lstlisting}
  Fetching the data is possible when the node is present in the data tree. If the node does not exist then a corresponding error is thrown. Also the watch argument can be used to leave a watch on the node which is triggered when the node is modified.
  
  \subsubsection{Deleting a Node}
  \begin{lstlisting}
    def delete(path, recursive=False)
  \end{lstlisting}
  If the node has children then the node can be deleted by setting the recursive argument to True. Otherwise the delete call throws an exception which indicates that the node is not empty.
  
  \subsubsection{Getting Children of a Node}
  \begin{lstlisting}
    def get_children(path, watch=None, include_data=False)
  \end{lstlisting}
  To list all the children of a node this call is used. It can also set a watch which notifies the client when new nodes are created below the node in question. By default, this call only returns the names of the child nodes. If the data of this node is also required then the include\_data parameter should be set to True.
  
  These are only some of the operations which are provided by the Kazoo library. The ones listed above are used as primitives in the construction of the ParKazoo library. There are also corresponding asynchronous function calls for each of these. The difference between them is that the asynchronous call does not immediately return the result. It instead returns a deferred object which can be used to notify the application when the result is ready. The is mostly used in single threaded evented programs.
  
\subsection{Library Recipes}
  ZooKeeper was designed to be simple. It does not provide higher order constructs like locks, queues or counters. It only provides the basic primitives which can be used to construct such functions. However the Kazoo library includes in itself some of the higher level constructs which can be used by the application programmer. Some of them are listed below.
  \subsubsection{Barrier}
    Barriers are synchronization mechanisms. Multiple ZooKeeper clients which have a common barrier will all block on the barrier until all the other clients with the same barrier also wait on it. Double Barrier is also implemented which also the start and end of a distributed task to be synchronized.
  \subsubsection{Counter}
  This is a synchronized counter which allows multiple nodes to share a counter value. A Counter type object can be used directly with the plus and minus operators.
  \subsubsection{Election}
  Multiple nodes can participate in election to become the leader. This construct allows the client to participate and then call a function when the client wins the election. There is also functionality to query for all the contenders in the election.
  \subsubsection{Locks}
    Locks are another synchronization mechanism. Only one client can acquire the lock. If the lock cannot be acquired then the client blocks till the lock is acquired. The acquirement of the lock can be also canceled Also all the contenders for the lock can be queried. Kazoo also provides Semaphore objects which are similar to the python Semaphores in the threading module. The Kazoo Semaphore object can be initialized with an initial count which indicates the number of available resources. Whenever one of the clients acquires the semaphore then the count is decremented. This way the number of client accessing the distributed resource can be regulated.
  \subsubsection{Partitioner}
  This construct is used to divide the members of a set among the members of a party, such that every member receives zero or more items and each item is only given to one member. This is generally used to divide tasks among distributed client nodes.
  \subsubsection{Party}
    Parties are used to keep a registry of nodes. Client nodes may enter or leave a Party which updates the membership registry of the party.
  \subsubsection{Queue}
    Kazoo supports simple Queues and a improved implementation called LockingQueue which provides locking and priority support. The simple Queue can be used to add items into the queue. The consumers of the queue can remove items from the queue. However if the the consumer client which acquired an item from the node crashes then the queue item is lost even though it might not have been processed. The LockingQueue provides items to consumers but does not remove the item from the queue. It creates an ephemeral node corresponding to the item which indicates that the item is being processed. In case the consumer client crashes then the ephemeral node is deleted and another consumer client can acquire the item. Once the consumer finishes processing the item it should explicitly inform the queue that it has processed the queue item at which time the queue removes the item from the queue.
  \subsubsection{Watchers}
  A client may need to watch for only certain types of changes to a node, such as the data of the node or its children. The watches left on the nodes by the client calls like get, get\_children or set are triggered whenever the node is updates irrespective of the type of event. The watchers provide more fine grained control. They can also be disabled by returning a False Boolean function from the callback function.
  
