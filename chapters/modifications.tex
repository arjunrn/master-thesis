\chapter{Implementation}

\section{ParKazoo Architecture}
The entire ParKazoo server set is called a ParKazoo ensemble. An ensemble contains multiple ZooKeeper clusters. When a client wants to connect to a ParKazoo ensemble it has to know the addresses of all the clusters, which could be all the servers of the cluster or a subset. If the information about all the clusters are not given to the client then the client cannot function correctly. For this reason the addresses of servers in the clusters needs to be stored centrally where it is accessible to all clients.  

\addvspace{1em}
\input{diagrams/parkazoo}
\addvspace{1em}
Figure ~\ref{fig:ParKazooArchitecture} depicts a ParKazoo ensemble with three clusters, with each cluster having three servers. All the clients are connected to all the cluster. However each client is connected to only one server in the cluster.
  	
\section{Client Initialization}
The information about all the constituent clusters are stored on one cluster which is part of the ParKazoo ensemble or on separate ZooKeeper cluster. The choice of the storage location is irrelevant as long as all the clients can access it. The configuration information could also be stored on all the clusters of the ParKazoo ensemble but this would require that the information is kept consistent. When the client has to be initialized only one of the server addresses has to be provided. The client connects to the server, reads the information about all the clusters and then initializes the individual Kazoo clients with the addresses of all the individual clusters.

\section{Mapping of Nodes}
As previously discussed the namespace of the data is divided between the clusters of the ParKazoo ensemble. To do this a node has to be mapped to a single cluster. The mapping procedure must always return the same cluster. This way the two different clients will always know where to place the node and where it can be retrieved from.

\input{algorithms/node_mapping}

Algorithm ~\ref{algo:nodeMapping} shows the algorithm to map nodes to clusters. The most important step in this procedure is to produce a hash value of the path string. The  SHA-256 algorithm is used for this purpose. This can be replaced with any other algorithm which has the same properties. The replacement hashing scheme should always the same produce the same output on different platforms for the same path. To place a \textit{znode} in the data tree the mapping function is used to find the cluster. For the lookup of this node the mapping function gives the clusters on which the \textit{znode} and the data for it can be retrieved.


\section{ParKazoo Operations}
Every ParKazoo operation consists of some Kazoo operations wrapped in a single ParKazoo operation. Figure ~\ref{fig:ParKazooAlgo} depicts the basic algorithm which is common to all ParKazoo operations. The algorithm begins by accepting the path of the znode to be operated upon and the type of operation. Then the algorithm establishes if the parent znode exists on the cluster of the target znode. If it does not exists then algorithm creates it if the operation is a modifying operating like \textit{create}. Otherwise an \textit{Error} is returned. Finally the operation is performed and the result of the operation is returned. 

\input{diagrams/parkazoo_algo.tex}

\subsection{Node Creation}
Before a node is created the parent needs to be checked. If the parent is an ephemeral node then an error is reported. The cluster of the parent node is obvious from the path of the child node. The call to create the node also contains flags to indicate if the node is an ephemeral node and/or a sequential node. If the node doesn't exist then a the path of nodes up to that point are also created.

\vspace{1.5em}
\input{diagrams/parkazoo_operations/creation}

\begin{lstlisting}
  def create(self, path, value=b"empty", ephemeral=False, sequence=False, makepath=False, acl=None):
    try:
        p_value, stat = p_cluster.get(parent_path)
        if stat.owner_session_id is not None:
            raise NoChildrenForEphemeralsError
    except NoNodeError:
        pass
    else:
        makepath = True
    clusters = self._get_path_clients(path)
    cluster = self._hash_function(clusters, path)
    return cluster.create(path, value=value, ephemeral=ephemeral, sequence=sequence, makepath=makepath, acl=acl)
\end{lstlisting}

The Figure~\ref{fig:PKCreateOp} shows the steps in the creation of a node. The node to be created \textit{/node2/child1}. The node \textit{child1} and its parent \textit{node2} maybe present on 2 different clusters. If the \textit{make\_path} argument is not set to True then the parent node \textit{node2} should exist. The destination cluster of \textit{child1} maybe different from its parent \textit{node2}. In that case the \textit{node2} needs to be created on the destination cluster in the first step. This is done with the \textit{makepath} argument. Then the node is created with the create call.

\subsection{Ensuring Path}
The Kazoo library provides an API call to ensure that a path exists. If the path does not exist then it is created recursively. This approach is also used in ParKazoo.

\begin{lstlisting}
def ensure_path(self, path, acl=None):
  parent, child = ospath.split(path)
  if parent is not '/':
      self.ensure_path(parent, acl=acl)

  parent_clusters = self._get_path_clients(parent)
  parent_cluster = self._hash_function(parent_clusters, parent)
  _, parent_stat = parent_cluster.get(parent)
  if parent_stat.owner_session_id is not None:
      raise NoChildrenForEphemeralsError

  clusters = self._get_path_clients(path)
  cluster = self._hash_function(clusters, path)
  return cluster.ensure_path(path, acl)
\end{lstlisting}

\subsection{Check for Node Existence}
The original functionality from Kazoo is reused after it has been wrapped in the mapping to find the right cluster.

\begin{lstlisting}
  def exists(self, path, watch=None):
    clusters = self._get_path_clients(path)
    cluster = self._hash_function(clusters, path)
    return cluster.exists(path, watch)
\end{lstlisting}

\subsection{Getting Node Data}
Fetching the data for a node is straightforward. Hash the parent path of the node and find the cluster to which the node is mapped. Then perform the \textit{get} using the Kazoo client for that cluster to fetch the data.

\begin{lstlisting}
  def get(self, path, watch=None):
    clusters = self._get_path_clients(path)
    cluster = self._hash_function(clusters, path)
    return cluster.get(path, watch)
\end{lstlisting}


\subsection{Setting Value of Node}
To set the value of a node find the cluster to which the node is mapped. Then using the Kazoo client for that cluster perform the set operation. 

\begin{lstlisting}
  def set(self, path, value, version=-1):
    clusters = self._get_path_clients(path)
    cluster = self._hash_function(clusters, path)
    return cluster.set(path, value=value, version=version)
\end{lstlisting}

\subsection{Finding children of a Node}
In the first we using the \textit{exists} call we ensure that the node for which the children are requested actually exists. Then we check if the parent node exists on the cluster which corresponds to the cluster for its child znodes. We also ensure the path(create the parent znode) in case of future requests which will trigger the watch function when child znodes are created at some point in the future.

\begin{lstlisting}
  def get_children(self, path, watch=None, include_data=False):
    node_exists = self.exists(path)
    clusters = self._get_path_clients(path)
    full_path = _prefix_root(self.chroot, path)
    full_path = '%s/' % full_path if not full_path.endswith('/') else full_path
    hashobj = hashlib.md5()
    hashobj.update(full_path.encode('UTF-8'))
    value = int(hashobj.hexdigest(), 16)
    selected_cluster = value % len(sorted(self.c.items()))
    clusters = self._get_path_clients(path)
    children_cluster = clusters[selected_cluster]
    try:
       return children_cluster.get_children(path, watch=watch, include_data=include_data)
    except NoNodeError:
       if node_exists:
           children_cluster.ensure_path(path)
           return children_cluster.get_children(path, watch=watch, include_data=include_data)
       else:
           raise
\end{lstlisting}

\input{diagrams/parkazoo_operations/get_children}

In the Figure~\ref{fig:getChildrenOperation} a query is made to the node \textit{/node2} to find its children. The children of \textit{/node2} maybe present on another cluster. In that case the parent node's existence has to be confirmed on the destination cluster. If the node does not exist then it has to be created. This is done so that watches can be left on the node in case nodes are created later. The the \textit{/node2} is queried on the destination cluster. Figure~\ref{subfig:getChildInit} shows the operation. The left side is the tree structure in the cluster which contains the node \textit{/node2}. It also has a sibling \textit{/node1}. The children of \textit{/node2} will get mapped to the cluster on the right. When there is a request to find the children for \textit{/node2} then a corresponding node is created on the children cluster. A watch is left on this newly created node. At some point of time in the future, as show in Figure~\ref{subfig:getChildCreate} when the node \textit{/node2/child1} is created the watch set on the node in the previous step is triggered.

\subsection{Transactions}
Transactions are used on ZooKeeper to speed up a sequence of operations. Either all the operations of a transaction are committed or the whole transaction is rolled back. This ensures that operations can be completed because only a single consensus is required to complete the whole transaction rather than an individual consensus for each operation in the transaction.
    
But in a ParKazoo ensemble there are multiple clusters and it's not possible to perform a consensus where the transaction is operating on znodes which are present on different clusters. Although this appears to be a substantial disadvantage, most transactions operate on sibling nodes. Later in the evaluation of the recipes we show these transactions are still used. Since all siblings are present on the same cluster in the case of strong consistency transactions can still be used to process the operations. In fact, most complex recipes like Locks, Barriers, Double Barriers which use transactions can still continue operating with this limitation.
    
To implement the transaction the original Kazoo Transaction object is wrapped with basic verification logic which ensures the operations of the transaction are all operating on the same cluster. Every time an operation is added to the transaction the cluster corresponding to that operation is noted. Before the commit occurs the list of clusters is verified so that it contains only a single cluster.
    
The \textit{Transaction} object provides the following operations:
\begin{enumerate}
	\item \lstinline | def create(path, value='', ephemeral=False, sequence=False) |
	\item \lstinline | def delete(path, version=-1) |
	\item \lstinline | def set_data(path, value, version=-1) |
	\item \lstinline | def check(path, version) |
	\item \lstinline | def commit_async() |
	\item \lstinline | def commit() |
\end{enumerate}