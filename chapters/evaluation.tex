\chapter{Evaluation}

The peak write throughput performance and the corresponding latencies of the original ZooKeeper and ParKazoo are compared in our evaluation. The tests are performed on a cluster(machine pool) of 50 machines. Each machine is equipped with an Intel\textsuperscript{\textregistered}  Xeon\textsuperscript{\texttrademark}  E5405 CPU clocked at 2.00GHz and 8GB System Memory. All the machines have a etXtreme BCM5754 Gigabit Ethernet PCI Express network card. For the purpose of these tests we use the ZooKeeper version 3.4.

Each machine has its own local storage as well as a networked storage which is shared and common between all nodes in the cluster. The executable binaries and scripts are stored on the shared storage so that they are available from all the machines in the cluster. The configuration files for each node is stored on the local storage.

The machines in the cluster are monitored using Ganglia~\cite{sacerdoti2003wide}. The Fabric library is used to orchestrate and coordinates the steps of the testing process. Fabric is a library for application deployment~\cite{spotswood2003systems} and system administration.

\section{Test Introduction}
The design goals for ParKazoo was to support higher throughput rates than  what is currently supported through ZooKeeper. In order to test if we have achieved our goal, we test for one simple metric. We find the maximum load, that is the maximum number of operations per second(read or write) which ParKazoo can support. We use the same test for ZooKeeper to compare and determine the improvement which was gained. For this we setup a ZooKeeper cluster or a ParKazoo ensemble of a certain size on some nodes in the machine pool. On the other nodes of the machine pool we run processes that execute the tests and determine the benchmark.

\section{Test Setup}
The first step is to start the required the server set, either ZooKeeper or ParKazoo and configure them. Then the machines which make the requests are are configured. Conceptually the create and delete operations are equivalent because they both require consensus between a quorum of the servers. Hence both of them are considered as write operations. The requester nodes create and delete \textit{znodes} on the servers. The created nodes are not retained on the server because the performance of the cluster gradually diminishes as the amount of data stored increases. 

The machines in the machine pool have one of the following role:
\subsection{Orchestrating Node}
Only a single machine is used to perform this role. It is in charge of starting the machines which perform the other roles. It also coordinates the tasks of collecting and aggregating the results and finally cleanup. Algorithm~\ref{algo:testOrchestration} shows the algorithm of the main orchestrating process.
\subsection{Server Nodes}
The server nodes run the ZooKeeper server processes. They are either configured to run in the regular ZooKeeper configuration or as a ParKazoo ensemble.
\subsection{Client Nodes}
The test client nodes run a Python script which forks and creates P number of slave processes as shown in Algorithm~\ref{algo:testClientMaster}. Each slave process in turn spawns T number of threads which is shown in Algorithm~\ref{algo:testClientProcess}. Each threads get its own client object which it can use to make requests to the main ensemble. Algorithm~\ref{algo:testClientThread} summarizes the actions performed in a test client thread. A ParKazoo/Kazoo barrier is used to wait for all the processes to initialise and spawn their threads. Once all the processes reach this barrier and they cross it and begin to execute a loop. The loops contains a \textit{create} to create a node in the ParKazoo/ZooKeeper tree structure. The size of the created znode is 1MB. Upon successful creation of the node the thread records the the current time and also the time it took to execute the operation. After that it removes the same node using the \textit{delete} operation. After this it records the timestamps and execution time just like for the create operation.

After a prefixed amount of time has elapsed the value of the finish node is set on the cluster. All the test client threads have a \textit{DataWatch} on this node. When the value is set it indicates to the threads to stop executing the requests loop. After this every processes collects the timestamps-duration pair lists from every thread and writes it to disk. 


\input{algorithms/tests/orchestration}

\input{algorithms/tests/client_master}

\input{algorithms/tests/client_process}

\input{algorithms/tests/client_thread}

\section{Testing Process}
The objective of the testing process is to find the peak throughput and the corresponding latencies. The setup consists an ensemble of servers. For ZooKeeper there is a cluster with three servers and for ParKazoo there is an ensemble of three clusters with three servers each. While it would be expected that a 9-Server ParKazoo ensemble should be compared against a 9-Server ZooKeeper cluster, it is more fair to compare it against a 3-Server ZooKeeper cluster. Both the three server ZooKeeper cluster and nine server cluster can tolerate a single server failure. Also since the performance of a cluster is inversely proportional to the size of cluster, the fastest ZooKeeper configuration is a three server configuration. This is an advantage which a nine server ParKazoo ensemble inherently possesses. The procedure for the tests is described as follows:

\begin{enumerate}
	\item In the beginning a single client node is initialized to make requests. It contains only a single thread with its own client. The average throughput for a thread count is recorded. Then the number of threads is incremented and the throughput for every thread count is recorded. The number of threads is incremented till there is no increase in the throughput. The throughput starts decreasing after a certain number of threads. The number of threads for which maximum throughput is obtained is used in the next step of the testing process. This gives T which is the maximum number of threads which can be run before the performance degrades due to context switching and other threading limitations.
	\item Continuing with a single testing node the test is repeated with T number of threads and an increasing number of processes. The number of processes is incremented and the throughput is measured for each process count. The throughput again decreases after a certain process count. This process count tipping point gives P which is number of processes for maximum throughput on a single node.
	\item Finally we test with multiple testing nodes. Each node has T threads and P number of processes. For T number of threads and P number of processes increase the number of test nodes until maximum throughput is achieved.
\end{enumerate}

Multiple nodes make requests to the ZooKeeper cluster or the ParKazoo ensemble. Each node has multiple processes and each process has in turn multiple threads. Each thread has its own Kazoo or ParKazoo client object to make requests to the servers. Kazoo/ParKazoo provides a recipe for barrier which is used by the clients to initialize and wait. Once all the client nodes reach the barrier it is cleared. To do this there is a common counter which is incremented whenever the client finishes initializing. The main orchestrating process waits for this counter to reach the precomputed value. Then it clears the barrier. This way all the client nodes can start making their requests at the same time. Every request is recorded with a timestamp and the time it took to complete the request. All these timestamped requests from each thread are collected and finally they are written to the disk. At the end of the run all the files containing the requests timestamp and duration are collected. They are then aggregated and processed. They are bucketed by the second in which they were executed. This gives the number of requests from all the test nodes that occurred in a second. The average request rate and their latency is computed and recorded. 

\input{charts/throughput_1_1_1}

\input{charts/requests_1_1_1}

The Figure~\ref{fig:single_node_throughput} shows the throughput over a period of 100 seconds for a single node with a single thread and process. As we can see the throughput is better for ParKazoo than for ZooKeeper. The latencies for the requests are show in Figure~\ref{chart:single_node_latencies}. The latencies for ParKazoo are in the range of 200 milliseconds to 300 milliseconds. Where as for ZooKeeper they are spread across the entire range from 100 to 600 milliseconds. From these initial reading we can assume that the performance for ParKazoo is nominally better than for an equivalent ZooKeeper setup for the minimal configuration.

\input{tables/write_single_process}

Table~\ref{table:single_process_write_throughput} lists the throughput for a single testing node with a single process. The throughput for ParKazoo is initially higher. But it does not scale linearly as more threads are added. Around the 80 threads mark the throughput for ZooKeeper increases over that of ParKazoo. In fact after more than 110 threads are added to the ParKazoo the process crashes due to the high number of open socket connections. This is of course an operating system limitation but exposes one of the flaws in the design of ParKazoo, that of unscalability. However this situation is unlikely to occur in a production system. The throughput of ZooKeeper increases as additional threads are added to a thread count of 210 threads. For ParKazoo the thread count of 110 is chosen as the highest thread count.

\input{tables/write_single_node}

Then in the next step the number of processes is increased. The Table~\ref{table:single_node_write_throughput} lists the average throughput for a single node with increasing number of processes for both ZooKeeper and ParKazoo. The Table~\ref{table:single_node_zookeeper_throughput} shows the throughput for ZooKeeper. For eleven processes the maximum throughput is reached. According to Table~\ref{table:single_node_parkazoo_throughput} the maximum throughput for ParKazoo is with 8 processes.

Then the test is repeated with the number of threads and processes determined and multiple test nodes. The Table~\ref{table:multinode_write_throughput_all} lists the throughputs for the tests. From the Table~\ref{table:zookeeper_multinode_write_throughput} the maximum throughput for ZooKeeper is achieved with 5 nodes. The maximum average throughput is 5421.84 requests/second. From Table~\ref{table:parkazoo_multinode_write_throughput} the maximum possible throughput for ParKazoo is 11430.90 requests/second. That is not the maximum possible throughput achievable When the tests were conducted with more nodes the clients started disconnecting because of the maximum number of connections which is supported had been reached. The infrastructure cannot support more concurrent connections.

\input{tables/write_multi_node}

The throughput rate for the test for both ZooKeeper and ParKazoo are shown in the Figure~\ref{fig:throughput_time_all}. The Figure~\ref{chart:max_thp_latencies} shows the latencies for requests from the same tests. The latencies for the ZooKeeper from Figure~\ref{chart:zk_max_latency} is in the range of 1.5 seconds to 1.75 seconds. And for ParKazoo from Figure~\ref{chart:parkazoo_max_latency} the latency for the requests is in the range of 250 milliseconds to 1 second. The performance of ParKazoo is better both in terms of throughput and request latencies.

\input{charts/throughput_65_10_15}
\input{charts/requests_65_10_15}

\section{Read Performance}
For pedantry's sake we also compare the performance of the read operation. The test for the read performance is the same as the writes. But instead of create and delete operation the read operation is done in a loop. When the test client threads are initialised they also create a few hundred znodes with 1MB of data which are then read in the loop. Table ~\ref{table:single_process_read_throughput} shows the performance for a single process with an increasing number of threads. The highest throughput is for 18 threads in ZooKeeper and 20 threads in ParKazoo. The experiments are repeated with increasing number of processes and the results are listed in Table~\ref{table:single_node_read_throughput} . For ZooKeeper the highest read throughput is for 18 processes and for ParKazoo with 20 processes. Finally the number of test client nodes are increased incrementally with the results in Table~\ref{table:multinode_read_throughput_all}. For ZooKeeper the highest throughput achieved is 145461.05 reads/seconds for 6 nodes. But the test for ParKazoo again runs into the problem os disconnections due to high number of open connections but the highest achieved read throughput is 304516.43 requests/second for 16 nodes. This is twice the throughput achieved for ZooKeeper. Although there is no conclusive result the initial measurements prove promising.
\input{tables/read_single_process}
\input{tables/read_single_node}
\input{tables/read_multi_node}

\section{Conclusion}
As we can see from the throughput rates when the number of clients is smaller the throughput of the ParKazoo is almost 100\%-120\% higher than the equivalent ZooKeeper setup. However when the number of ParKazoo clients on a single node increases the throughput rate fails to grow and at a certain point drops below the throughput rate of the the equivalent ZooKeeper test node. But the total throughput of ZooKeeper plateaus out at around 6300 requests/seconds. However the request throughput of ParKazoo reached around 11000 requests/second in our tests.

The reason for the deteriorated performance of ParKazoo when more processes and threads are present is probably due to the higher number of open connections. This issue has need to be investigated further. 
