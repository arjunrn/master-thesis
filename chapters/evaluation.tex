\chapter{Evaluation}

The peak write throughput performance of the original ZooKeeper and ParKazoo are compared. The tests are performed on a Ganglia cluster of 50 machines. Each machine is equipped with an Intel\textsuperscript{\textregistered}  Xeon\textsuperscript{\texttrademark}  E5405 CPU clocked at 2.00GHz and 8GB System Memory. All the machines have a etXtreme BCM5754 Gigabit Ethernet PCI Express network card.

\section{Test Setup}
The tests are performed on a Ganglia~\cite{sacerdoti2003wide} cluster. The Fabric library is used to orchestrate and perform the tests. Fabric is a library for application deployment~\cite{spotswood2003systems} and system administration. The first step is to start the required the server set, either ZooKeeper or ParKazoo and configure them. Then the machines which make the requests are are configured next. Henceforth they are referred to as \textit{Requesters}. The Requesters run a python script which forks and creates P number of slave processes. Each slave process in turn spawns T number of threads. Each threads get its own client object which it can use to make requests to the main ensemble. A ParKazoo/Kazoo barrier is used to wait for all the processes to initialize and spawn their threads. Once all the processes reach this barrier and cross the begin to execute a loop. The loops contains a \textit{create} to create a node in the ParKazoo/ZooKeeper tree structure. Upon successful creation of the node the thread records the the current time and also the time it took to execute the operation. After that it removes the same node using the \textit{delete} operations. After this it records the timestamps and execution time just like for the create operation. Conceptually the create and delete operations are equivalent because they both require consensus between a majority of the servers. The created nodes are not retained on the server because the performance of the cluster gradually diminishes as the amount of data it contains increases. After a prefixed amount of time has elapsed the value a node is set on the cluster. All the \textit{Requesters} have a \textit{DataWatch} on this node. When the value is set it indicates to the \textit{Requesters} to stop executing the requests loop. After this every processes collects the timestamps-duration pair lists from every thread and writes it to disk. 

\subsection{Testing Process}
The objective of the testing process is to find the peak throughput and the corresponding latencies. The setup consists an ensemble of servers. For ZooKeeper there is a cluster with three servers and for ParKazoo there is an ensemble of three clusters with three servers each. While it would be expected that a 9-Server ParKazoo ensemble should be compared against a 9-Server ZooKeeper cluster, it is more fair to compare it against a 3-Server ZooKeeper cluster. Both the three server ZooKeeper cluster and nine server cluster can tolerate a single server failure. Also since the performance of a cluster is inversely proportional to the size of cluster, the fastest ZooKeeper configuration is a three server configuration. This is an advantage which a nine server ParKazoo ensemble inherently possesses. The procedure for the tests is described as follows:

\begin{enumerate}
	\item In the beginning a single client node is initialized to make requests. It contains only a single thread with its own client. The average throughput for a thread count is recorded. Then the number of threads is incremented and the throughput for every thread count is recorded. The number of threads is incremented till there is no increase in the throughput. The throughput starts decreasing after a certain number of threads. The number of threads for which maximum throughput is obtained is used in the next step of the testing process. This gives T which is the maximum number of threads which can be run before the performance degrades due to context switching and other threading limitations.
	\item Continuing with a single testing node the test is repeated with T number of threads and an increasing number of processes. The number of processes is incremented and the throughput is measured for each process count. The throughput again decreases after a certain process count. This process count tipping point gives P which is number of processes for maximum throughput on a single node.
	\item Finally we test with multiple testing nodes. Each node has T threads and P number of processes. For T number of threads and P number of processes increase the number of test nodes until maximum throughput is achieved.
\end{enumerate}

Multiple nodes make requests to the ZooKeeper cluster or the ParKazoo ensemble. Each node has multiple processes and each process has in turn multiple threads. Each thread has its own Kazoo or ParKazoo client object to make requests to the servers. Kazoo/ParKazoo provides a recipe for barrier which is used by the clients to initialize and wait. Once all the client nodes reach the barrier it is cleared. To do this there is a common counter which is incremented whenever the client finishes initializing. The main orchestrating process waits for this counter to reach the precomputed value. Then it clears the barrier. This way all the client nodes can start making their requests at the same time. Every request is recorded with a timestamp and the time it took to complete the request. All these timestamped requests from each thread are collected and finally they are written to the disk. At the end of the run all the files containing the requests timestamp and duration are collected. They are then aggregated and processed. They are bucketed by the second in which they were executed. This gives the number of requests from all the test nodes that occurred in a second. The average request rate and their latency is computed and recorded. 

\input{charts/throughput_1_1_1}

\input{charts/requests_1_1_1}

The Figure~\ref{fig:single_node_throughput} shows the throughput over a period of 100 seconds for a single node with a single thread and process. As we can see the throughput is better for ParKazoo than for ZooKeeper. The latencies for the requests are show in Figure~\ref{chart:single_node_latencies}. The latencies for ParKazoo are in the range of 200 milliseconds to 300 milliseconds. Where as for ZooKeeper they are spread across the entire range from 100 to 600 milliseconds. From these initial reading we can assume that the performance for ParKazoo is nominally better than for an equivalent ZooKeeper setup for the minimal configuration.

\input{tables/single_process}

Table~\ref{table:single_process_throughput} lists the throughput for a single testing node with a single process. The throughput for ParKazoo is initially higher. But it does not scale linearly as more threads are added. Around the 80 threads mark the throughput for ZooKeeper increases over that of ParKazoo. In fact after more than 110 threads are added to the ParKazoo the process crashes due to the high number of open socket connections. This is of course an operating system limitation but exposes one of the flaws in the design of ParKazoo, that of unscalability. However this situation is unlikely to occur in a production system. The throughput of ZooKeeper increases as additional threads are added to a thread count of 210 threads. For ParKazoo the thread count of 110 is chosen as the highest thread count.

\input{tables/single_node}

Then in the next step the number of processes is increased. The Table~\ref{table:single_node_throughput} lists the average throughput for a single node with increasing number of processes for both ZooKeeper and ParKazoo. The Table~\ref{table:single_node_zookeeper_throughput} shows the throughput for ZooKeeper. For eleven processes the maximum throughput is reached. According to Table~\ref{table:single_node_parkazoo_throughput} the maximum throughput for ParKazoo is with 8 processes.

Then the test is repeated with the number of threads and processes determined and multiple test nodes. The Table~\ref{table:multinode_throughput_all} lists the throughputs for the tests. From the Table~\ref{table:zookeeper_multinode_throughput} the maximum throughput for ZooKeeper is achieved with 5 nodes. The maximum average throughput is 5421.84 requests/second. From Table~\ref{table:parkazoo_multinode_throughput} the maximum possible throughput for ParKazoo is 11430.90 requests/second. That is not the maximum possible throughput achievable When the tests were conducted with more nodes the clients started disconnecting because of the maximum number of connections which is supported had been reached. The infrastructure cannot support more concurrent connections.

\input{tables/multi_node}

The throughput rate for the test for both ZooKeeper and ParKazoo are shown in the Figure~\ref{fig:throughput_time_all}. The Figure~\ref{chart:max_thp_latencies} shows the latencies for requests from the same tests. The latencies for the ZooKeeper from Figure~\ref{chart:zk_max_latency} is in the range of 1.5 seconds to 1.75 seconds. And for ParKazoo from Figure~\ref{chart:parkazoo_max_latency} the latency for the requests is in the range of 250 milliseconds to 1 second. The performance of ParKazoo is better both in terms of throughput and request latencies.

\input{charts/throughput_65_10_15}
\input{charts/requests_65_10_15}

\section{Conclusion}
As we can see from the throughput rates when the number of clients is smaller the throughput of the ParKazoo is almost 100\%-120\% higher than the equivalent ZooKeeper setup. However when the number of ParKazoo clients on a single node increases the throughput rate fails to grow and at a certain point drops below the throughput rate of the the equivalent ZooKeeper test node. But the total throughput of ZooKeeper plateaus out at around 6300 requests/seconds. However the request throughput of ParKazoo reached around 11000 requests/second in our tests.

The reason for the deteriorated performance of ParKazoo when more processes and threads are present is probably due to the higher number of open connections. This issue has need to be investigated further. 
