\chapter{Related Work}
Camelot~\cite{hastings1990distributed} is early implementation for the Mach operating system which proposed a locking service using transactions in a distributed environment. Using Camelot application developers can build \textit{encapsulated, shared and recoverable} objects. This system does not restrict the size of the data being shared. It's also unique that it is one of the first system which implemented the coordination and synchronization completely in software and could be run on different types of hardware.

Boxwood~\cite{maccormick2004boxwood} presents a distributed storage medium with an emphasis on fault-tolerance. Boxwood differs from similar earlier system due it's use of higher level data abstractions as synchronization primitives. Previous attempts to synchronize data across a distributed storage focused on lower level data abstractions like disk blocks. Boxwood instead used a B-Tree~\cite{skiena504algorithm} to store data.

dynamoDB~\cite{decandia2007dynamo} is a distributed Key-Value store which can be configured so that its \textit{Consistency} and \textit{Availability} properties can be varied according to the CAP Theorem~\cite{Gilbert:2002:BCF:564585.564601}. The objective of dynamoDB was to provide a highly available data store even in the presence of node failures. But this requires that the any data consistency issues must be handled by the developer on a per application policy basis. dynamoDB also uses Consistent Hashing~\cite{Karger:1997:CHR:258533.258660} so as to reduce the amount of data migration and equally distributed the migration load in case of node failure. 

Any discussion of ZooKeeper is incomplete without mentioning Chubby~\cite{burrows2006chubby}. Chubby was developed to perform some of the same functions as ZooKeeper with a few minor exceptions. Chubby was initially designed to be a locking service which provided coarse-grained locks to its clients. The locks are advisory and are leased for a specified amount of time. Secondly, although Chubby delivers change events to clients asynchronously, the developers of Chubby also implemented client side caching to mitigate the effect of polling. Chubby exposes a file-system like data structure to its clients. Chubby has become a core component of several large-scale distributed applications like the Google File System~\cite{Ghemawat:2003:GFS:945445.945450} and MapReduce~\cite{Dean:2008:MSD:1327452.1327492}.

At the heart of every coordination service like ZooKeeper is a consensus algorithm. Paxos~\cite{lamport2001paxos} is the most popular consensus algorithm and modified versions of it have been implemented in Chubby~\cite{burrows2006chubby} and other services including ZooKeeper. Most consensus algorithm are based on the idea of State Machine Replication~\cite{schneider1990implementing}. 

The Chandra-Toueg Algorithm~\cite{Chandra:1996:UFD:226643.226647} is another consensus algorithm which is designed to handle Byzantine faults which is overlooked by Paxos. The biggest drawback of Paxos is that it is hard to understand and there have been attempts to simplify the algorithm~\cite{chandra2007paxos}~\cite{lampson2001abcd}. Raft~\cite{ongaro2013search} is an attempt to create a consensus algorithm which is easy to understand and can be more easily used to build coordination services.

ZooKeeper uses the ZAB broadcast algorithm~\cite{junqueira2011zab}. The ZAB algorithm ensures any state changes which are broadcast are received in the order they are broadcast. ZAB also is used in the election of the primary which processes the state change requests.

More recently Flavio Juqueira, one of the creators of ZooKeeper proposal a partitioned implementation~\cite{junqueira2010partitioned} of ZooKeeper. This proposal is similar to our implementation However it also proposes another component on the main ensemble which routes the requests from the clients to the destination clusters. The distribution of the znodes can also be dynamic, which means that the client which creates a znode can specify at runtime which cluster the nodes gets mapped to. This complicates the use of the system because the application developer needs to keep compute and map nodes to individual clusters. This proposal also relies on the assumption that the requests are always distributed uniformly across the tree. In case of hotspot the tree has to be split and spread across multiple ensembles.

There was also an attempt to modify the ZooKeeper core~\cite{biligiri2014multiquorum} itself with multiple quorums. Instead of single leader in a quorum, every server has the chance to be the leader for a subset of the data. The author's argument is that this would lead to more uniform utilization of resources in case of hotspots. 
