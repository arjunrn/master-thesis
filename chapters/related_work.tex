\chapter{Related Work}
\section{Earlier and Related Systems}
Camelot~\cite{hastings1990distributed} is early implementation for the Mach operating system which proposed a locking service using transactions in a distributed environment. Using Camelot application developers could build \textit{encapsulated}, textit{shared} and \textit{recoverable} objects. This system does not restrict the size of the data being shared. It's also unique in that it is one of the first system which implemented the coordination and synchronization completely in software and could be run on different types of hardware.

Boxwood~\cite{maccormick2004boxwood} presents a distributed storage medium with an emphasis on fault-tolerance. Boxwood differs from similar earlier system due it's use of higher level data abstractions as synchronization primitives. Previous attempts to synchronize data across a distributed storage focused on lower level data abstractions like disk blocks. Boxwood instead used a B-Tree~\cite{skiena504algorithm} to store data.

dynamoDB~\cite{decandia2007dynamo} is a distributed Key-Value store which can be configured so that its \textit{Consistency} and \textit{Availability} properties can be varied according to the CAP Theorem~\cite{Gilbert:2002:BCF:564585.564601}. The objective of dynamoDB was to provide a highly available data store even in the presence of node failures. But this requires that the any data consistency issues must be handled by the developer on a per application policy basis. dynamoDB also uses Consistent Hashing~\cite{Karger:1997:CHR:258533.258660} so as to reduce the amount of data migration and equally distributed the migration load in case of node failure. 

\section{Similar Systems}
Any discussion of ZooKeeper is incomplete without mentioning Chubby~\cite{burrows2006chubby}. Chubby was developed to perform some of the same functions as ZooKeeper with a few minor exceptions. Chubby was initially designed to be a locking service which provided coarse-grained locks to its clients. The locks are advisory and are leased for a specified amount of time. Secondly, although Chubby delivers change events to clients asynchronously, the developers of Chubby also implemented client side caching to mitigate the effect of polling. Chubby exposes a file-system like data structure to its clients. Chubby has become a core component of several large-scale distributed applications like the Google File System~\cite{Ghemawat:2003:GFS:945445.945450} and MapReduce~\cite{Dean:2008:MSD:1327452.1327492}.

More recently Flavio Juqueira, one of the creators of ZooKeeper proposed a partitioned implementation~\cite{junqueira2010partitioned} of ZooKeeper. This proposal is similar to our implementation However it also includes another component on the main ensemble which routes the requests from the clients to the destination clusters. The distribution of the znodes can also be dynamic, which means that the client which creates a znode can specify at runtime to which cluster the nodes gets mapped. This complicates the use of the system because the application developer needs to keep compute and map nodes to individual clusters. This proposal also relies on the assumption that the requests are always distributed uniformly across the tree. In case of hotspotsin a part of the tree then that part of the tree has to be split and spread across multiple ensembles.

There was also an attempt to modify the ZooKeeper core~\cite{biligiri2014multiquorum} itself with multiple quorums. Instead of single leader in a quorum, every server has the chance to be the leader for a subset of the data. The author's argument is that this would lead to more uniform utilization of resources in case of hotspots.

\section{Coordination Protocols}

At the heart of every coordination service like ZooKeeper is a consensus algorithm. Paxos~\cite{lamport2001paxos} is the most famous and common consensus algorithm and modified versions of it have been implemented in Chubby~\cite{burrows2006chubby} and other services including ZooKeeper. Most consensus algorithm are based on the idea of State Machine Replication~\cite{schneider1990implementing}. 

The Chandra-Toueg Algorithm~\cite{Chandra:1996:UFD:226643.226647} is another consensus algorithm which is designed to handle Byzantine faults which is overlooked by Paxos. The biggest drawback of Paxos is that it is hard to understand and there have been attempts to simplify the algorithm~\cite{chandra2007paxos}~\cite{lampson2001abcd}. Raft~\cite{ongaro2013search} is an attempt to create a consensus algorithm which is easy to understand and can be more easily used to build coordination services.

ZooKeeper uses the ZAB broadcast algorithm~\cite{junqueira2011zab}. The ZAB algorithm ensures any state changes which are broadcast are received in the order they are broadcast. ZAB also is used in the election of the primary which processes the state change requests.


